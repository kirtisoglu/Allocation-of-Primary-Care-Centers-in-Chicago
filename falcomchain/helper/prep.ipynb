{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import pickle\n",
    "from data_handler import DataHandler\n",
    "from falcomchain.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = DataHandler()\n",
    "chicago = handler.load_chicago()\n",
    "chicagooo = handler.load_chicagooo()\n",
    "#centers = handler.load_centers()\n",
    "graph = handler.load_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chicago = gpd.read_file('data/chicago.json')\n",
    "#center_lat, center_lon = projection(chicago)  # see gerrychain library for that function. \n",
    "#fig =  plot.fig_blocks(chicago)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Graph of Census Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirtisoglu/Documents/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/falcomchain/graph/graph.py:265: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  areas = df.geometry.area.to_dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' lookup()  #Lookup a node/field attribute. Returns the value of the attribute field at node.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" We use Gerrychain library to construct a dual graph of Census blocks. \"\"\"\n",
    "\n",
    "# Write a function to create a graph removing default/secified columns from a dataset\n",
    "\n",
    "#graph = Graph.from_geodataframe(chicago)\n",
    "#nx.draw(graph,pos={node:(graph.nodes()[node]['centroid'].x,graph.nodes()[node]['centroid'].y) for node in graph.nodes()}, node_size=20)\n",
    "#graph.to_json(\"GraphToJson.json\")\n",
    "\n",
    "# graph = Graph.from_json(\"data/GraphToJson.json\")\n",
    "\n",
    "# Opening JSON file\n",
    "#f = open('GraphToJson.json')\n",
    "# returns JSON object as a dictionary\n",
    "#data = json.load(f)\n",
    "#f.close()\n",
    "#data.keys()\n",
    "\n",
    "\"\"\"adjacency = random.choice(list(data['adjacency']))\n",
    "adjacency\"\"\"\n",
    "\n",
    "\"\"\"node = random.choice(list(graph.nodes))\n",
    "graph.nodes[node]['GEOID20'] \"\"\"\n",
    "\n",
    "\"\"\"first_node = list(graph.nodes)[0]\n",
    "node_attributes = graph.nodes[first_node]\n",
    "print(f\"Attributes for node {first_node}: {node_attributes}\")\"\"\"\n",
    "\n",
    "\"\"\" lookup()  #Lookup a node/field attribute. Returns the value of the attribute field at node.\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The areas of the polygons are included as node attributes (with key `area`).\n",
    "        \n",
    "- The shared perimeter of neighboring polygons are included as edge attributes (with key `shared_perim`).\n",
    "\n",
    "- Nodes corresponding to polygons on the boundary of the union of all the geometries (e.g., the state, if your dataframe describes VTDs) have a    `boundary_node` attribute (set to `True`) and a `boundary_perim` attribute\n",
    "with the length of this \"exterior\" boundary.\n",
    "\n",
    "- Gerrychain library has centroid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boundary_node': False,\n",
       " 'area': 9898.599340102353,\n",
       " 'GEOID20': '170310625001004',\n",
       " 'C_X': -87.67235840971288,\n",
       " 'C_Y': 41.94260543133003,\n",
       " 'population': 76,\n",
       " 'candidate': 0,\n",
       " 'real_candidate': 0,\n",
       " 'artificial_candidate': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = random.choice(list(graph.nodes))\n",
    "graph.nodes[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirtisoglu/Documents/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/falcomchain/graph/graph.py:265: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  areas = df.geometry.area.to_dict()\n"
     ]
    }
   ],
   "source": [
    "def induce_graph(graph):\n",
    "    \n",
    "    graphhh = Graph.from_geodataframe(chicagooo)\n",
    "    \n",
    "    u = random.choice(list(graph.nodes))\n",
    "    v = random.choice(list(graphhh.nodes))\n",
    "    attr_list = set(graph.nodes[u])\n",
    "    induced_attr_list = set(graphhh.nodes[v])\n",
    "    redundant_attr_list = induced_attr_list -  attr_list\n",
    "    \n",
    "    for node in graphhh.nodes:\n",
    "        graphhh.nodes[node]['population'] = graph.nodes[node]['population']\n",
    "        graphhh.nodes[node]['C_X'] = graph.nodes[node]['C_X']\n",
    "        graphhh.nodes[node]['C_Y'] = graph.nodes[node]['C_Y']\n",
    "        graphhh.nodes[node]['candidate'] = graph.nodes[node]['candidate']\n",
    "        graphhh.nodes[node]['real_candidate'] = graph.nodes[node]['real_candidate']\n",
    "        \n",
    "        for attr in redundant_attr_list:\n",
    "            del graphhh.nodes[node][attr] \n",
    "    return graphhh\n",
    "    \n",
    "graphhh = induce_graph(graph)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boundary_node', 'area', 'GEOID20', 'C_X', 'C_Y', 'population', 'candidate', 'real_candidate', 'artificial_candidate']\n"
     ]
    }
   ],
   "source": [
    "u = random.choice(list(graph.nodes))\n",
    "print(list(graph.nodes[u]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boundary_node': False,\n",
       " 'area': 10174.400307323747,\n",
       " 'GEOID20': '170313106002003',\n",
       " 'population': 32,\n",
       " 'C_X': -87.68720631755467,\n",
       " 'C_Y': 41.87470464989468,\n",
       " 'candidate': 1,\n",
       " 'real_candidate': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = random.choice(list(graphhh.nodes))\n",
    "\n",
    "graphhh.nodes[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from falcomchain.helper import save_pickle\n",
    "\n",
    "path = \"/Users/kirtisoglu/Documents/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/data/processed/graphhh.pkl\"\n",
    "save_pickle(graphhh, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17855"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from falcomchain.helper import load_pickle\n",
    "\n",
    "g = load_pickle(path)\n",
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Populations from DHC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dhc = pd.read_csv('data/dhc_2020.csv')\n",
    "#pop_dict = load.get_column(data = dhc, column = 'TOT_POP')  # returns {key: geoid, value: population}\n",
    "#load.add_to_graph(graph, pop_dict)  # adds 'population' attributes to nodes\n",
    "#load.add_to_data(chicago, pop_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See in API that\n",
    "\n",
    "classgerrychain.partition.GeographicPartition\n",
    "\n",
    "crosses_parts(edge: Tuple)→ bool\n",
    "\n",
    "flip(flips: Dict)→ Partition\n",
    "\n",
    "plot(geometries=None, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random PHCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phc_df, selected_blocks = load.random_phc(chicago)\n",
    "#candidates = load.add_phc(chicago, graph, selected_blocks)\n",
    "#fig = plot.add_phc(fig, phc_df)\n",
    "#fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Travel Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Correct the projection to get the precise walking times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"travel_times calculated and saved. Travel time betwween two blocks is fixed as 10 minutes.\"\n",
    "handler = DataHandler()\n",
    "graph = handler.load_graph()\n",
    "candidates = handler.load_candidates()\n",
    "\n",
    "import pickle\n",
    "import gzip \n",
    "\n",
    "file_path = '/Users/kirtisoglu/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/prepared_data/newtravel.pkl.gz'\n",
    "with gzip.open(file_path, 'wb') as file:\n",
    "                pickle.dump(t, file)\n",
    "\n",
    "for u, v in graph.edges():\n",
    "    graph[u][v]['time'] = 10\n",
    "t = travel.travel_time(graph, candidates)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Travel Time Mode: Walking Time with the Shortest Distance\"\n",
    "\n",
    "import math\n",
    "\n",
    "# the shortest distance in km over the earth’s surface using the ‘Haversine’ formula.\n",
    "def get_distance_from_lat_lon_in_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of the earth in km\n",
    "    dLat = deg2rad(lat2 - lat1)\n",
    "    dLon = deg2rad(lon2 - lon1)\n",
    "    a = (\n",
    "        math.sin(dLat / 2) * math.sin(dLat / 2) +\n",
    "        math.cos(deg2rad(lat1)) * math.cos(deg2rad(lat2)) *\n",
    "        math.sin(dLon / 2) * math.sin(dLon / 2)\n",
    "    )\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = R * c # Distance in km\n",
    "    return d\n",
    "\n",
    "def deg2rad(deg):\n",
    "    return deg * (math.pi / 180)\n",
    "\n",
    "speed = 4.5  # walking speed in km/h for pedestrians (considering traffic lights. See the the urban design section on wiki page of Preferred_walking_speed: typical 5.0 km/h, recommended 4.8 km/h)\n",
    "# Calculate distances between block centroids and assign them as edge weights\n",
    "for edge in graph.edges:\n",
    "    u, v = edge\n",
    "    lat1, lon1 = graph.nodes[u]['C_Y'], graph.nodes[u]['C_X']\n",
    "    lat2, lon2 = graph.nodes[v]['C_Y'], graph.nodes[v]['C_X']\n",
    "    \n",
    "    weight_hours = get_distance_from_lat_lon_in_km(lat1, lon1, lat2, lon2) / speed # walking time in hours\n",
    "    weight = weight_hours * 60 * 60  # convert to seconds\n",
    "    \n",
    "    graph.edges[edge]['walk'] = math.floor(weight) # no need for miliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shared_perim': 0.00045513733859732183,\n",
       " 'time': 10,\n",
       " 'random_weight': 0.07680618364019531,\n",
       " 'walk': 107}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge = random.choice(list(graph.edges))\n",
    "graph.edges[edge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/kirtisoglu/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/prepared_data/graph_new.pkl'\n",
    "with open(path, 'wb') as file:\n",
    "                pickle.dump(graph, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from falcomchain.helper import DataHandler\n",
    "\n",
    "handler = DataHandler()\n",
    "candidates = handler.load_candidates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = handler.load_candidates()\n",
    "\n",
    "import pickle\n",
    "import gzip \n",
    "\n",
    "attr = 'walk'\n",
    "\n",
    "t = travel.travel_time(graph, candidates, attr=attr)\n",
    "\n",
    "file_path = '/Users/kirtisoglu/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/prepared_data/travel_walking.pkl.gz'\n",
    "with gzip.open(file_path, 'wb') as file:\n",
    "                pickle.dump(t, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Surprisingly, it uses only 10MB memory, while GraphToJson.json uses 27MB.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" 1) Graph is saved as graph.pkl in prepared_data folder.\"\n",
    "# load.cache_data(graph, file_path='prepared_data/graph.pkl', method = 'pickle')  \n",
    "\" Surprisingly, it uses only 10MB memory, while GraphToJson.json uses 27MB.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 2) Travel times is saved as travel_times.pkl.gz\n",
    "    ujson is loaded in 35 sec, and uses 140MB \n",
    "    pkl.gz is loaded in 16-17 sec, and uses 130MB memory. \"\"\"\n",
    "# load.cache_data(travel_times, file_path='prepared_data/travel_times.pkl.gz', method = 'pickle_gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 3) Chicago data is saved as chicago.pkl.\n",
    "    pkl is loaded in 1 sec, and uses 19MB memory. \n",
    "    Original chicago.json file uses 38MB.\n",
    "    I didn't really get it. pkl is better than json in both memory and loading.\"\"\"\n",
    "    \n",
    "# load.cache_data(chicago, file_path='prepared_data/chicago.pkl', method='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 4) Possible PHC locations (candidates) are saved as candidates.pkl\"\"\"\n",
    "# load.cache_data(candidates, file_path='prepared_data/candidates.pkl', method='pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Real Libraries and Schools Data (Real Facility Candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = DataHandler()\n",
    "graph = handler.load_graph()\n",
    "newtravel = handler.load_newtravel()\n",
    "chicago = handler.load_chicago()\n",
    "geo_candidates = handler.load_geo_candidates()\n",
    "candidates = handler.load_candidates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_candidates = {32681, 516, 2973, 24279, 29, 32306, 8289}\n",
    "for node in artificial_candidates:\n",
    "    graph.nodes[node]['candidate'] = 1\n",
    "    graph.nodes[node]['artificial_candidate'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_candidates = {32681, 516, 2973, 24279, 29, 32306, 8289}\n",
    "\n",
    "for node in graph.nodes:\n",
    "\n",
    "    if graph.nodes[node]['candidate']== 1:\n",
    "        \n",
    "        if int(node) in artificial_candidates:\n",
    "            graph.nodes[node]['artificial_candidate'] = 1\n",
    "            graph.nodes[node]['real_candidate'] = 0\n",
    "            \n",
    "        else:\n",
    "            graph.nodes[node]['artificial_candidate'] = 0\n",
    "            graph.nodes[node]['real_candidate'] = 1\n",
    "    else:\n",
    "        graph.nodes[node]['real_candidate'] = 0\n",
    "        graph.nodes[node]['artificial_candidate'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chicago candidate info from true to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "for node in candidates:\n",
    "    print(\"candidate, real, artificial\",graph.nodes[node]['candidate'], graph.nodes[node]['real_candidate'], graph.nodes[node]['artificial_candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph.nodes:\n",
    "    if graph.nodes[node]['candidate']== 1:\n",
    "        if graph.nodes[node]['artificial_candidate'] == 1:\n",
    "            graph.nodes[node]['real_candidate'] = 0\n",
    "        else:\n",
    "            graph.nodes[node]['real_candidate'] = 1\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'real_0': 38828,\n",
       " 'real_1': 693,\n",
       " 'artificial_0': 39514,\n",
       " 'artificial_1': 7,\n",
       " 'candidate_0': 38821,\n",
       " 'candidate_1': 700}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = {'real_0':0, 'real_1':0, 'artificial_0':0, 'artificial_1':0, 'candidate_0':0, 'candidate_1':0}\n",
    "\n",
    "for node in graph.nodes:\n",
    "    \n",
    "    if graph.nodes[node]['candidate']== 1:\n",
    "        count['candidate_1'] += 1\n",
    "    else:\n",
    "        count['candidate_0'] += 1\n",
    "        \n",
    "        \n",
    "    if graph.nodes[node]['artificial_candidate'] == 1:\n",
    "        count['artificial_1'] += 1\n",
    "    if graph.nodes[node]['artificial_candidate']==0:\n",
    "        count['artificial_0'] += 1\n",
    "\n",
    "\n",
    "    \n",
    "    if graph.nodes[node]['real_candidate'] == 1:\n",
    "        count['real_1'] += 1\n",
    "    if graph.nodes[node]['real_candidate']==0:\n",
    "        count['real_0'] += 1\n",
    "\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from falcomchain.helper import save_pickle\n",
    "path = '/Users/kirtisoglu/Documents/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/data/processed/graph.pkl'\n",
    "save_pickle(graph, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lately, we relaize that there is no enough candidates aroung Chicago O'Hare Airport.\n",
    "# We select some census blocks manually from the map, save their centroids as artifical candidates\n",
    "# and merge them into geo_canidates.\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"Artificial candidates\"\n",
    "artificial_candidates = {32681, 516, 2973, 24279, 29, 32306, 8289}\n",
    "\n",
    "# Create a new GeoDataFrame for artificial_candidates\n",
    "artificial_candidates_data = {\n",
    "    'block': list(artificial_candidates),\n",
    "    'geometry': [chicago.loc[block, 'centroid'] for block in artificial_candidates]\n",
    "}\n",
    "\n",
    "geo_artificial_candidates = gpd.GeoDataFrame(artificial_candidates_data, geometry='geometry')\n",
    "\n",
    "geo_candidates['artificial'] = False\n",
    "geo_artificial_candidates['artificial'] = True \n",
    "\n",
    "# Set the CRS for geo_artificial_candidates\n",
    "geo_artificial_candidates.set_crs(geo_candidates.crs, inplace=True)\n",
    "\n",
    "# Concatenate the new GeoDataFrame with geo_candidates\n",
    "geo_candidates = pd.concat([geo_candidates, geo_artificial_candidates], ignore_index=True)\n",
    "\n",
    "# Reset index of geo_candidates if needed\n",
    "geo_candidates.reset_index(drop=True, inplace=True)\n",
    "\n",
    "import pickle\n",
    "file_path = '/Users/kirtisoglu/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/prepared_data/geo_artificial_candidates.pkl'\n",
    "with open(file_path, 'wb') as file:\n",
    "                pickle.dump(geo_artificial_candidates, file)\n",
    "\n",
    "file_path = '/Users/kirtisoglu/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/prepared_data/geo_candidates.pkl'\n",
    "with open(file_path, 'wb') as file:\n",
    "                pickle.dump(geo_candidates, file)\n",
    "                \n",
    "########\n",
    "\n",
    "# edit other datasets              \n",
    "for node in artificial_candidates:\n",
    "    graph.nodes[node]['real_phc'] = True\n",
    "\n",
    "file_path = '/Users/kirtisoglu/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/prepared_data/ggraph.pkl'\n",
    "with open(file_path, 'wb') as file:\n",
    "                pickle.dump(graph, file)\n",
    "\n",
    "\n",
    "from travel_time import travel_time_to_source\n",
    "\n",
    "for source in artificial_candidates:\n",
    "    to_source = travel_time_to_source(graph, source)\n",
    "    for i in graph.nodes:\n",
    "        newtravel[(i, source)] = to_source[i]\n",
    "        \n",
    "file_path = '/Users/kirtisoglu/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/prepared_data/newtravel.pkl.gz'\n",
    "with open(file_path, 'wb') as file:\n",
    "                pickle.dump(newtravel, file)\n",
    "                \n",
    "candidates = candidates.union(artificial_candidates)\n",
    "file_path = '/Users/kirtisoglu/Documents/GitHub/Allocation-of-Primary-Care-Centers-in-Chicago/prepared_data/candidates.pkl'\n",
    "with open(file_path, 'wb') as file:\n",
    "                pickle.dump(candidates, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Areas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We scale utm areas in such a way that density of a spanning tree is 1.\n",
    "\n",
    "def scale_areas(n_teams, pop_col=None):\n",
    "    # change the name of the population attribute\n",
    "    pop_target = pop / n_teams\n",
    "    pop = sum(graph.nodes[node][pop_col] for node in graph.nodes)\n",
    "    sum_utm = sum(graph.nodes[node]['area'] for node in graph.nodes)\n",
    "    factor = pop / sum_utm\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        graph.nodes[node]['area_scaled'] = graph.nodes[node]['area']*factor \n",
    "        \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Allocation-of-Primary-Care-Centers-in-Chicago (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
